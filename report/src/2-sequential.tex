\chapter{Sequential}
\label{cap:sequential}
In this chapter we present the main aspect of the sequential implementation to count the $n$-grams in an input, that is in \href{https://github.com/edoardosarri24/parallel-trigrams/blob/cf8e82b27f908c5c836d37a205fc04d8d7fed5be/sequential}{sequential} GitHub folder.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Pipeline}
Let's look the pipeline of our sequential algorithm. This was implemented in \href{https://github.com/edoardosarri24/parallel-trigrams/blob/c57c7fc52c9bcaa8fb9ba1403b795b0793713a49/sequential/src/main.c}{main.c} file.
\begin{enumerate}
    \item We take the text input file, that must be named as \textit{input.txt} and must be in \textit{data} directory, and we pre-precessing it. The implementation, that is \href{https://github.com/edoardosarri24/parallel-trigrams/blob/cf8e82b27f908c5c836d37a205fc04d8d7fed5be/sequential/src/my_utils.c}{my\_utils.c} file: write all the characters except the space and the punctuation; between two token we write ony a single space. After this step we have a normalized corpus in \textit{data/normalized\_file.txt}.
    \item We allocate memory for the hash table.
    \item We iterate over all the $n$-gram in this normalized file and stop when there aren't another $n$-gram to analyse.
    \begin{enumerate}
        \item Find the next $n$-gram with \href{https://github.com/edoardosarri24/parallel-trigrams/blob/cf8e82b27f908c5c836d37a205fc04d8d7fed5be/sequential/src/my_utils.c}{next\_ngram} function.
        \item Use the \texttt{add\_gram} function in \href{https://github.com/edoardosarri24/parallel-trigrams/blob/cf8e82b27f908c5c836d37a205fc04d8d7fed5be/sequential/src/hash_table.c}{hash\_table.c} file to add the $n$-gram in out hash table. This function increase the related counter if current $n$-gram match one already found; althought we insert as new node in the chain and initialize the counter to one. At the end it return with the cursor in the initial position (relative to this function call) of the corpus.
        \item Move \textit{STRIDE} words forward in the corpus.
    \end{enumerate}
    \item Colects and prints the statistics.
    \item Release the memory allocated for the hash table.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hash table}
The major complexity si in the \href{https://github.com/edoardosarri24/parallel-trigrams/blob/cf8e82b27f908c5c836d37a205fc04d8d7fed5be/sequential/src/hash_table.c}{hash\_table.c} file that modeling the hash table structure and its operation.

Now we see the main characteristics of this implementation:
\begin{itemize}
    \item In the Section~\ref{sec:overflow} we said that we must using almost a 32bits variable to avoid the overflow of intermediate value during the index calculation. To obtain this goal we have used the \textit{uint\_fast32\_t} C type that garantee the faster type that have almost 32bits.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Statistics}
To extract the statistics described in Section~\ref{sec:statistics}, we implemented two function in \href{https://github.com/edoardosarri24/parallel-trigrams/blob/master/sequential/src/statistics.c}{statistics.c}.

\subsection{Text statistics}
For the text statistics, we explored two different approaches.

\subsubsection{Two steps approach}
The algorithm followed these steps:
\begin{enumerate}
    \item Traverse the whole hash table structure to count the total number of unique $n$-grams ($N$).
    \item Allocate a temporary array of \texttt{Node} pointers of size $N$. This allocation was obtained using the \texttt{malloc} function. Using a Variable Length Array (VLA) allocates memory on the stack, which is faster but limited in size. Using \texttt{malloc} allocates memory on the heap, which is necessary when $N$ is unknown or large to avoid stack overflow.
    \item Populate the array with the nodes from the hash table.
    \item Sort the entire array using the standard C library function \texttt{qsort}, with a custom comparator that orders nodes by frequency in descending order.
    \item Print the top $K$ elements from the sorted array and the count of unique $n$-grams.
    \item Free the allocated array.
\end{enumerate}

\subsubsection{Top-K approach}
This is the algorithm that we implemented in our code.
\begin{enumerate}
    \item Allocate a small array \textit{top\_ngrams} of pointers of size $TOP\_K$, i.e., the number of top elements requested.
    \item Iterate through the whole hash table only once. For each unique $n$-gram:
    \begin{enumerate}
        \item Increment the counter of unique $n$-grams.
        \item If the \texttt{top\_ngrams} array is not yet full, add the current $n$-gram to it.
        \item If the array is full, find the element within \texttt{top\_ngrams} that has the minimum frequency.
        \item Compare the current $n$-gram's frequency with this minimum. If the current $n$-gram is more frequent, it replaces the minimum element in the array.
    \end{enumerate}
    \item Finally, sort the \texttt{top\_ngrams} array in descending order and print the results.
\end{enumerate}

\subsubsection{Complexity Analysis}
Let $N$ be the number of unique $n$-grams and $K$ be the number of top elements to find.

\begin{itemize}
    \item \textbf{Time Complexity} \\
        In the first approach the bottleneck is the sorting step, which takes $O(N\log N)$. In our implementation we pass through the hash table only once, so we have $O(N)$; for each $n$-gram we pass over $K$ elements, so in total we have $O(N \cdot K)$. Since is usually very small, the total time complexity is $O(N)$.
    \item \textbf{Space Complexity} \\
        The first approach requires $O(N)$ extra space for the temporary array. The seond approach requires only $O(K)$ that is irrilevant for small $K$.
\end{itemize}

\subsection{Hash table performance}
For the hash table performance the pipeline is the following:
\begin{enumerate}
    \item Initialize counters, \textit{total\_elements}, \textit{busy\_buckets}, and \textit{max\_chain\_len}, to 0.
    \item Iterate through the whole \textit{buckets} array of the hash table.
    \item For each bucket:
    \begin{enumerate}
        \item If the bucket is not empty increment \textit{busy\_buckets}.
        \item Traverse the open chain of bucket to count the nodes.
        \item Update \textit{max\_chain\_len} if the current chain length is greater than the current maximum.
        \item Add the chain length to \texttt{total\_elements}.
    \end{enumerate}
    \item Print the results.
\end{enumerate}

\subsubsection{Complexity Analysis}
The time complexity is $O(M+N)$, where $M$ is the number of buckets and $N$ is the total number of unique $n$-grams: we visit every bucket and every node exactly once.

The space complexity is $O(1)$: we only use a few variables for counting.